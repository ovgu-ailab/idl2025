---
layout: default
title: Reading Assignment
id: reading6
---


# Reading Assignment: Attention and Memory

## General Reading

Start with the [Bishop Book - Section 12.1](https://www.bishopbook.com/).
A complementary overview on the topic is provided by 
<!-- the blog post ["Attention and Memory in Deep Learning and NLP" by Denny Britz](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/) -->
the blog post ["Attention? Attention!" by Lilian Weng](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html).
Finally, read the distill.pub article
["Attention and Augmented Recurrent Neural Networks" by Olah & Carter](https://distill.pub/2016/augmented-rnns/).


## More on Transformers (optional)

* For a detailed visual explanation of Transformers, visit the blog post ["The Illustrated Transformer" by Jay Alammar](https://jalammar.github.io/illustrated-transformer/).

* Lilian Weng provides a very comprehensive overview on architectures derived from the original Transformer in the blog post ["The Transformer Family"](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html).

* Alternatively, you can read Chapter 11.4 of the Book ["Deep Learning with Python, Second Edition"](https://www.manning.com/books/deep-learning-with-python-second-edition).

* Optionally, also have a look at [Dehghani et al. "Universal Transformers"](https://arxiv.org/abs/1807.03819) - This follow-up of the original transformer paper (see further reading below) is much more accessible and also incorporates the idea of adaptive compute time.

* Optionally, if you are more into videos, there is a [lecture by Alex Graves on "Attention and Memory in Deep Learning"](https://www.youtube.com/watch?v=AIiwuClvH6k) that covers many of the topics from this week's reading assignments.

## More on neural networks with explicit memory (optional)

If you would like to dive more into this very exciting topic. have a look at the paper [Graves et al. "Hybrid computing using a neural network with dynamic external memory" in Nature 538.7626 (2016): pp. 471-476.](https://www.nature.com/articles/nature20101.epdf).
There is also a [video recording of Alex Graves' talk at NIPS 2016](https://www.youtube.com/watch?v=steioHoiEms).


## Further Reading (Optional)

There are many exciting directions to explore further:

* Follow the links from the blog post to the referenced articles for further reading.
* The original transformer paper [Ashish et al. "Attention Is All You Need" in NIPS (2017)](https://arxiv.org/abs/1706.03762) (The version on arXiv seems to be most recent.)